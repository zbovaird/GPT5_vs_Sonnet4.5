{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNB9cIB5Zmi5SX6mIbOYe2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zbovaird/GPT5_vs_Sonnet4.5/blob/main/UHG_IDS_v4_8_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRXJhokRakZH",
        "outputId": "de503e83-26f1-4f8e-9b85-c6141a6fa989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install PyTorch Geometric (matches your current torch/cuda)\n",
        "!pip -q install --upgrade pip\n",
        "import torch\n",
        "pt = torch.__version__.split('+')[0]\n",
        "cuda = torch.version.cuda\n",
        "if torch.cuda.is_available() and cuda:\n",
        "  idx = f\"https://data.pyg.org/whl/torch-{pt}+cu{cuda.replace('.','')}.html\"\n",
        "else:\n",
        "  idx = f\"https://data.pyg.org/whl/torch-{pt}+cpu.html\"\n",
        "\n",
        "!pip -q install torch_scatter torch_sparse torch_cluster torch_spline_conv -f {idx}\n",
        "!pip -q install torch_geometric scikit-learn scipy pandas tqdm\n",
        "\n",
        "# Install PyNNDescent for ultra-fast approximate KNN\n",
        "!pip -q install pynndescent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Intrusion Detection using Universal Hyperbolic Geometry (UHG) v4.8.1\n",
        "🔬 PyNNDescent + Class Weighting @ 10% Data (APPLES-TO-APPLES vs v4.5!)\n",
        "\n",
        "v4.8.1 TESTING CONFIGURATION:\n",
        "- 🎯 PURPOSE: Isolate PyNNDescent's impact on training quality\n",
        "- PyNNDescent for approximate KNN (vs sklearn in v4.5)\n",
        "- 10% data sampling (283k samples) - SAME as v4.5 for comparison!\n",
        "- CLASS WEIGHTED LOSS - SAME as v4.5!\n",
        "- k=2, PCA, all settings IDENTICAL to v4.5 except KNN method\n",
        "\n",
        "Why v4.8.1 @ 10%:\n",
        "- v4.8 @ 20% showed extreme class weights broke BENIGN detection\n",
        "- v4.5 @ 10% with weighting worked well (95.51% accuracy)\n",
        "- Need to test: Does PyNNDescent (approximate KNN) affect accuracy vs sklearn (exact)?\n",
        "- This is the ONLY way to fairly compare KNN methods!\n",
        "\n",
        "Expected vs v4.5:\n",
        "- v4.5 @ 10%: sklearn KNN (~70s), good accuracy (95.51%)\n",
        "- v4.8.1 @ 10%: PyNN KNN (~4-5s), SAME accuracy? (test this!)\n",
        "- If accuracy matches: PyNNDescent is 10-15x faster with NO quality loss! 🚀\n",
        "- If accuracy drops: Approximate KNN trades accuracy for speed ⚠️\n",
        "\n",
        "PyNNDescent Details:\n",
        "- Algorithm: Nearest Neighbor Descent (approximate)\n",
        "- Accuracy: 95-98% of exact KNN (per library docs)\n",
        "- Speedup: 10-50x faster than sklearn\n",
        "- Question: Does 2-5% KNN approximation error affect GNN training?\n",
        "\n",
        "Class Weighting Strategy:\n",
        "- Inverse frequency weighting (same as v4.5)\n",
        "- At 10% scale, weights are less extreme than 20%\n",
        "- Expected to work well based on v4.5 results\n",
        "\n",
        "v4.8.1 Features:\n",
        "- CLASS WEIGHTED LOSS for minority class detection\n",
        "- PyNNDescent for ultra-fast approximate KNN ⭐ TESTING!\n",
        "- PCA dimensionality reduction (77→20 dims, 89% variance)\n",
        "- k=2 neighbors (same as v4.5)\n",
        "- Fixed evaluation for missing classes in test set\n",
        "- Comprehensive timing instrumentation and bottleneck analysis\n",
        "- GPU detection and memory usage tracking\n",
        "- UHG constraint verification post-training\n",
        "\n",
        "Expected Performance @ 10%:\n",
        "- Data Loading:     ~20s\n",
        "- KNN (PyNNDescent): ~4-5s 🚀 (vs ~70s sklearn in v4.5!)\n",
        "- Training:         ~30-40s (fewer samples)\n",
        "- Total:            ~60-75s (1-1.5 min vs ~2 min in v4.5)\n",
        "\n",
        "Expected Accuracy @ 10%:\n",
        "- Overall: ~95-96% (matching v4.5 if PyNN doesn't hurt)\n",
        "- BENIGN: ~90-91% (matching v4.5)\n",
        "- Bot: ~85-90% recall\n",
        "- FTP-Patator: ~95-99%\n",
        "- SSH-Patator: ~95-99%\n",
        "- Macro F1: Unknown (v4.5 didn't report, but should be good)\n",
        "\n",
        "Compare to Previous Versions:\n",
        "v4.5 @ 10% (weighted, sklearn):  95.51% acc, ~70s KNN, ~2 min total\n",
        "v4.8.1 @ 10% (weighted, PyNN):   ???% acc, ~5s KNN, ~1 min total ← TESTING!\n",
        "v4.8 @ 20% (weighted):           80.56% acc (BROKEN by extreme weights)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import coo_matrix\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from typing import Tuple\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import platform\n",
        "from datetime import datetime\n",
        "\n",
        "# Import PyNNDescent\n",
        "from pynndescent import NNDescent\n",
        "\n",
        "# Optional: Drive mount (only in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Device configuration with detailed GPU info\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🖥️  HARDWARE CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
        "    cuda_version = torch.version.cuda\n",
        "    gpu_capability = torch.cuda.get_device_capability(0)\n",
        "\n",
        "    print(f\"✅ GPU Detected:\")\n",
        "    print(f\"   • Model: {gpu_name}\")\n",
        "    print(f\"   • Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"   • CUDA Version: {cuda_version}\")\n",
        "    print(f\"   • Compute Capability: {gpu_capability[0]}.{gpu_capability[1]}\")\n",
        "    print(f\"   • Device: cuda:0\")\n",
        "else:\n",
        "    print(f\"⚠️  No GPU available - using CPU\")\n",
        "    print(f\"   • This will be significantly slower for training\")\n",
        "\n",
        "print(f\"\\n🔬 Configuration (v4.8.1 - Apples-to-Apples vs v4.5):\")\n",
        "print(f\"   • KNN Method: PyNNDescent (Approximate) ⭐ TESTING!\")\n",
        "print(f\"   • Loss: CLASS-WEIGHTED CrossEntropyLoss ⚖️ (same as v4.5)\")\n",
        "print(f\"   • Data: 10% sampling (283k samples) - SAME AS v4.5!\")\n",
        "print(f\"   • k=2, PCA, all settings IDENTICAL to v4.5 except KNN\")\n",
        "print(f\"   • Expected KNN time: ~4-5s (vs ~70s sklearn in v4.5!)\")\n",
        "print(f\"   • Expected total time: ~60-75s (1-1.5 min)\")\n",
        "print(f\"   • Goal: Test if PyNN's 2-5% KNN error affects accuracy\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ======================\n",
        "# Configuration\n",
        "# ======================\n",
        "FILE_PATH = '/content/drive/MyDrive/CIC_data.csv'\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/uhg_ids_model_best.pth'\n",
        "RESULTS_PATH = '/content/drive/MyDrive/uhg_ids_results/'\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "def get_env_info():\n",
        "    return {\n",
        "        'python': platform.python_version(),\n",
        "        'torch': torch.__version__,\n",
        "        'cuda_available': torch.cuda.is_available(),\n",
        "        'device': str(device),\n",
        "    }\n",
        "\n",
        "# ===================\n",
        "# UHG Geometry Helpers\n",
        "# ===================\n",
        "\n",
        "def minkowski_inner_product(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Minkowski inner product: ⟨x,y⟩_M = ∑x_i*y_i - x_t*y_t\"\"\"\n",
        "    spatial = (x[..., :-1] * y[..., :-1]).sum(dim=-1)\n",
        "    time = x[..., -1] * y[..., -1]\n",
        "    return spatial - time\n",
        "\n",
        "def projective_normalize(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"Normalize projective coordinates: x² + y² - z² = -1\"\"\"\n",
        "    spatial_norm_sq = (x[..., :-1] ** 2).sum(dim=-1, keepdim=True)\n",
        "    z = torch.sqrt(torch.clamp(spatial_norm_sq + 1.0, min=eps))\n",
        "    return torch.cat([x[..., :-1], z], dim=-1)\n",
        "\n",
        "def uhg_quadrance_vectorized(x: torch.Tensor, y: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"Compute vectorized UHG quadrance between vectors\"\"\"\n",
        "    numerator = minkowski_inner_product(x, y)\n",
        "    denom_x = torch.clamp(-minkowski_inner_product(x, x), min=eps)\n",
        "    denom_y = torch.clamp(-minkowski_inner_product(y, y), min=eps)\n",
        "    cos_val = numerator / torch.sqrt(denom_x * denom_y)\n",
        "    cos_val = torch.clamp(cos_val, min=-1.0+eps, max=1.0-eps)\n",
        "    return 1 - cos_val**2\n",
        "\n",
        "def verify_uhg_constraints(x: torch.Tensor, name: str = \"embeddings\"):\n",
        "    \"\"\"Verify Minkowski norm constraints\"\"\"\n",
        "    norm_sq = minkowski_inner_product(x, x)\n",
        "    violation = torch.abs(norm_sq + 1.0)\n",
        "    max_viol = violation.max().item()\n",
        "    mean_viol = violation.mean().item()\n",
        "    print(f\"UHG Constraint Check ({name}):\")\n",
        "    print(f\"  Max violation: {max_viol:.6f}\")\n",
        "    print(f\"  Mean violation: {mean_viol:.6f}\")\n",
        "    if max_viol > 0.01:\n",
        "        print(f\"  ⚠️ WARNING: Constraints violated!\")\n",
        "    else:\n",
        "        print(f\"  ✅ Constraints satisfied\")\n",
        "\n",
        "# ====================\n",
        "# Data Loading\n",
        "# ====================\n",
        "\n",
        "def load_and_preprocess_data(file_path: str = FILE_PATH, sample_frac: float = 0.10) -> Tuple[torch.Tensor, torch.Tensor, dict, torch.Tensor, dict]:\n",
        "    \"\"\"\n",
        "    v4.8.1: Load and preprocess data (WITH class weighting, 10% sampling for v4.5 comparison)\n",
        "\n",
        "    Returns:\n",
        "        node_features: torch.Tensor of shape (n_samples, n_features)\n",
        "        labels_tensor: torch.Tensor of shape (n_samples,)\n",
        "        label_mapping: dict mapping label names to indices\n",
        "        class_weights: torch.Tensor of shape (num_classes,) - inverse frequency weights\n",
        "        timings: dict of timing information\n",
        "    \"\"\"\n",
        "    timings = {}\n",
        "\n",
        "    print(f\"\\nLoading data from: {file_path}\")\n",
        "    t0 = time.perf_counter()\n",
        "    data = pd.read_csv(file_path, low_memory=False)\n",
        "    timings['csv_read'] = time.perf_counter() - t0\n",
        "    print(f\"  ⏱️  CSV read: {timings['csv_read']:.2f}s\")\n",
        "\n",
        "    # Strip whitespace from column names and label values (matching v4.6)\n",
        "    t0 = time.perf_counter()\n",
        "    data.columns = data.columns.str.strip()\n",
        "    data['Label'] = data['Label'].str.strip()\n",
        "    timings['column_cleanup'] = time.perf_counter() - t0\n",
        "\n",
        "    unique_labels = data['Label'].unique()\n",
        "    print(f\"\\nUnique labels in the dataset: {unique_labels}\")\n",
        "    label_counts = data['Label'].value_counts()\n",
        "    print(\"\\nLabel distribution in the dataset:\")\n",
        "    print(label_counts)\n",
        "\n",
        "    # Simple random sampling\n",
        "    print(f\"\\nApplying random sampling (frac={sample_frac})...\")\n",
        "    t0 = time.perf_counter()\n",
        "    data_sampled = data.sample(frac=sample_frac, random_state=42)\n",
        "    timings['sampling'] = time.perf_counter() - t0\n",
        "    print(f\"  ⏱️  Sampling: {timings['sampling']:.2f}s\")\n",
        "\n",
        "    print(f\"\\nSampled label distribution:\")\n",
        "    sampled_label_counts = data_sampled['Label'].value_counts()\n",
        "    print(sampled_label_counts)\n",
        "\n",
        "    # Convert to numeric and handle missing values\n",
        "    t0 = time.perf_counter()\n",
        "    data_numeric = data_sampled.apply(pd.to_numeric, errors='coerce')\n",
        "    timings['to_numeric'] = time.perf_counter() - t0\n",
        "    print(f\"  ⏱️  Convert to numeric: {timings['to_numeric']:.2f}s\")\n",
        "\n",
        "    # Fill NaN and inf\n",
        "    t0 = time.perf_counter()\n",
        "    data_filled = data_numeric.fillna(data_numeric.mean())\n",
        "    data_filled = data_filled.replace([np.inf, -np.inf], np.nan)\n",
        "    data_filled = data_filled.fillna(data_filled.max())\n",
        "    if data_filled.isnull().values.any():\n",
        "        data_filled = data_filled.fillna(0)\n",
        "    timings['fillna'] = time.perf_counter() - t0\n",
        "    print(f\"  ⏱️  Fill NaN/inf: {timings['fillna']:.2f}s\")\n",
        "\n",
        "    labels = data_sampled['Label']\n",
        "    features = data_filled.drop(columns=['Label'])\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    timings['scaling'] = time.perf_counter() - t0\n",
        "    print(f\"  ⏱️  Scaling: {timings['scaling']:.2f}s\")\n",
        "\n",
        "    unique_labels = sorted(labels.unique())\n",
        "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    labels_numeric = labels.map(label_mapping).values\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    node_features = torch.tensor(features_scaled, dtype=torch.float32)\n",
        "    labels_tensor = torch.tensor(labels_numeric, dtype=torch.long)\n",
        "    timings['to_tensors'] = time.perf_counter() - t0\n",
        "    print(f\"  ⏱️  Convert to tensors: {timings['to_tensors']:.2f}s\")\n",
        "\n",
        "    print(\"\\nPreprocessing complete.\")\n",
        "    print(f\"Feature shape: {node_features.shape}\")\n",
        "    print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "\n",
        "    # Show class distribution for reference\n",
        "    class_counts = np.bincount(labels_numeric)\n",
        "    print(\"\\nClass distribution in processed data:\")\n",
        "    for label, idx in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
        "        count = class_counts[idx]\n",
        "        pct = (count / len(labels_numeric)) * 100\n",
        "        print(f\"  {label:30s}: {count:7d} samples ({pct:5.2f}%)\")\n",
        "\n",
        "    # Compute class weights (inverse frequency)\n",
        "    total_samples = len(labels_numeric)\n",
        "    num_classes = len(label_mapping)\n",
        "    class_weights = total_samples / (num_classes * class_counts)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    print(\"\\n⚖️  Class Weights (inverse frequency):\")\n",
        "    for label, idx in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
        "        weight = class_weights[idx]\n",
        "        print(f\"  {label:30s}: {weight:7.4f}\")\n",
        "\n",
        "    timings['total'] = sum(timings.values())\n",
        "    print(f\"\\n⏱️  Total data loading time: {timings['total']:.2f}s\")\n",
        "\n",
        "    return node_features, labels_tensor, label_mapping, class_weights_tensor, timings\n",
        "\n",
        "# =========================\n",
        "# Graph construction (KNN)\n",
        "# =========================\n",
        "\n",
        "def create_graph_data(node_features: torch.Tensor, labels: torch.Tensor, k: int = 2, use_pca: bool = True, pca_components: int = 20) -> Tuple[Data, dict]:\n",
        "    \"\"\"v4.8.1: PyNNDescent approximate KNN with PCA dimensionality reduction\"\"\"\n",
        "    timings = {}\n",
        "\n",
        "    print(\"\\nCreating graph structure...\")\n",
        "    t0 = time.perf_counter()\n",
        "    features_np = node_features.cpu().numpy()\n",
        "    timings['to_numpy'] = time.perf_counter() - t0\n",
        "\n",
        "    # PCA for faster KNN\n",
        "    if use_pca and features_np.shape[1] > pca_components:\n",
        "        print(f\"\\nApplying PCA for faster KNN...\")\n",
        "        print(f\"  • Original features: {features_np.shape[1]}\")\n",
        "        t0 = time.perf_counter()\n",
        "        pca = PCA(n_components=pca_components)\n",
        "        features_reduced = pca.fit_transform(features_np)\n",
        "        timings['pca'] = time.perf_counter() - t0\n",
        "        explained_var = pca.explained_variance_ratio_.sum()\n",
        "        print(f\"  • Reduced features: {features_reduced.shape[1]}\")\n",
        "        print(f\"  • Explained variance: {explained_var:.4f} ({explained_var*100:.2f}%)\")\n",
        "        print(f\"  ⏱️  PCA: {timings['pca']:.2f}s\")\n",
        "        features_for_knn = features_reduced\n",
        "    else:\n",
        "        features_for_knn = features_np\n",
        "        timings['pca'] = 0.0\n",
        "\n",
        "    # ===== v4.8.1: PyNNDescent Approximate KNN =====\n",
        "    print(f\"\\n🚀 Computing KNN graph with PyNNDescent (k={k})...\")\n",
        "    print(f\"  • Input shape: {features_for_knn.shape}\")\n",
        "    print(f\"  • Number of samples: {features_for_knn.shape[0]:,}\")\n",
        "    print(f\"  • Number of features: {features_for_knn.shape[1]}\")\n",
        "    print(f\"  • Using PyNNDescent (approximate)\")\n",
        "    print(f\"  • Algorithm: Nearest Neighbor Descent\")\n",
        "    print(f\"  • Expected: 10-20x faster than sklearn @ 10%!\")\n",
        "    print(f\"  • Accuracy: 95-98% of exact KNN\")\n",
        "\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    # Build PyNNDescent index\n",
        "    print(f\"  • Building NNDescent index...\")\n",
        "    index = NNDescent(\n",
        "        features_for_knn,\n",
        "        n_neighbors=k+1,  # +1 to include self, will remove later\n",
        "        metric='euclidean',\n",
        "        n_jobs=-1,  # Use all CPU cores\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Get neighbor indices (includes self as first neighbor)\n",
        "    indices, distances = index.neighbor_graph\n",
        "\n",
        "    # Remove self-connections (first column)\n",
        "    indices = indices[:, 1:]  # Skip first neighbor (self)\n",
        "\n",
        "    # Create edge index\n",
        "    num_nodes = features_for_knn.shape[0]\n",
        "    row = np.repeat(np.arange(num_nodes), k)\n",
        "    col = indices.flatten()\n",
        "\n",
        "    edge_index = torch.from_numpy(\n",
        "        np.vstack([row, col])\n",
        "    ).long().to(device)\n",
        "\n",
        "    timings['knn_computation'] = time.perf_counter() - t0\n",
        "    print(f\"  ✅ PyNNDescent KNN computation: {timings['knn_computation']:.2f}s\")\n",
        "    print(f\"  💡 Speedup vs sklearn (est ~70s @ v4.5): ~{70/timings['knn_computation']:.1f}x!\")\n",
        "\n",
        "    timings['edge_index_creation'] = 0.0  # Already included in knn_computation\n",
        "    print(f\"Edge index shape: {edge_index.shape}\")\n",
        "\n",
        "    # Add homogeneous coordinate (projective)\n",
        "    t0 = time.perf_counter()\n",
        "    node_features_uhg = torch.cat([\n",
        "        node_features.to(device),\n",
        "        torch.ones(node_features.size(0), 1, device=device)\n",
        "    ], dim=1)\n",
        "    timings['add_homogeneous'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    node_features_uhg = projective_normalize(node_features_uhg)\n",
        "    timings['projective_normalize'] = time.perf_counter() - t0\n",
        "    print(f\"  ⏱️  UHG projection: {timings['projective_normalize']:.2f}s\")\n",
        "\n",
        "    print(f\"Feature shape with homogeneous coordinate: {node_features_uhg.shape}\")\n",
        "\n",
        "    # Verify UHG constraints\n",
        "    t0 = time.perf_counter()\n",
        "    verify_uhg_constraints(node_features_uhg, name=\"initial features\")\n",
        "    timings['constraint_verification'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    total_samples = len(node_features_uhg)\n",
        "    indices_split = torch.randperm(total_samples)\n",
        "    train_size = int(0.7 * total_samples)\n",
        "    val_size = int(0.15 * total_samples)\n",
        "\n",
        "    train_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "    val_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "    test_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "\n",
        "    train_mask[indices_split[:train_size]] = True\n",
        "    val_mask[indices_split[train_size:train_size+val_size]] = True\n",
        "    test_mask[indices_split[train_size+val_size:]] = True\n",
        "    timings['split_creation'] = time.perf_counter() - t0\n",
        "\n",
        "    print(f\"\\nTrain size: {train_mask.sum()}, Val size: {val_mask.sum()}, Test size: {test_mask.sum()}\")\n",
        "\n",
        "    timings['total'] = sum(timings.values())\n",
        "    print(f\"\\n⏱️  Total graph construction time: {timings['total']:.2f}s\")\n",
        "\n",
        "    return Data(\n",
        "        x=node_features_uhg,\n",
        "        edge_index=edge_index,\n",
        "        y=labels.to(device),\n",
        "        train_mask=train_mask,\n",
        "        val_mask=val_mask,\n",
        "        test_mask=test_mask\n",
        "    ).to(device), timings\n",
        "\n",
        "# ==============================\n",
        "# UHG GraphSAGE Message Passing\n",
        "# ==============================\n",
        "\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "class UHGMessagePassing(MessagePassing):\n",
        "    def __init__(self, in_features: int, out_features: int):\n",
        "        super().__init__(aggr='add')\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight_msg = nn.Parameter(torch.Tensor(in_features, out_features))\n",
        "        self.weight_node = nn.Parameter(torch.Tensor(in_features, out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight_msg)\n",
        "        nn.init.xavier_uniform_(self.weight_node)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        # x includes homogeneous coord\n",
        "        # Transform node features (spatial only)\n",
        "        features = x[:, :-1]\n",
        "        z = x[:, -1:]\n",
        "        transformed_features = features @ self.weight_node\n",
        "        # Propagate using full projective vectors for weight computation\n",
        "        # Pass explicit size to handle all nodes (including isolated ones)\n",
        "        out = self.propagate(edge_index, x=x, size=(x.size(0), x.size(0)))\n",
        "        # Combine\n",
        "        out = out + transformed_features\n",
        "        # Recompute time-like to maintain Minkowski norm -1\n",
        "        out_full = torch.cat([out, z], dim=1)\n",
        "        out_full = projective_normalize(out_full)\n",
        "        return out_full\n",
        "\n",
        "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        # x_i, x_j are full projective vectors\n",
        "        weights = torch.exp(-uhg_quadrance_vectorized(x_i, x_j))\n",
        "        # Transform neighbor features (spatial only)\n",
        "        messages = (x_j[:, :-1]) @ self.weight_msg\n",
        "        return messages * weights.view(-1, 1)\n",
        "\n",
        "    def aggregate(self, inputs: torch.Tensor, index: torch.Tensor, ptr=None, dim_size=None) -> torch.Tensor:\n",
        "        # Sum messages per destination (with explicit dim_size to handle all nodes)\n",
        "        numerator = scatter_add(inputs, index, dim=0, dim_size=dim_size)\n",
        "        # Sum weights per destination (approximate by ones per feature dim)\n",
        "        weights_sum = scatter_add(torch.ones_like(inputs), index, dim=0, dim_size=dim_size)\n",
        "        return numerator / torch.clamp(weights_sum, min=1e-6)\n",
        "\n",
        "class UHGGraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, num_layers: int, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # in_channels includes homogeneous coord\n",
        "        actual_in = in_channels - 1\n",
        "        self.layers.append(UHGMessagePassing(actual_in, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(UHGMessagePassing(hidden_channels, hidden_channels))\n",
        "        self.layers.append(UHGMessagePassing(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        h = x\n",
        "        for layer in self.layers[:-1]:\n",
        "            h = layer(h, edge_index)\n",
        "            # Apply ReLU on spatial part only\n",
        "            spatial = F.relu(h[:, :-1])\n",
        "            h = torch.cat([spatial, h[:, -1:]], dim=1)\n",
        "            h = self.dropout(h)\n",
        "        h = self.layers[-1](h, edge_index)\n",
        "        return h[:, :-1]  # logits on spatial part\n",
        "\n",
        "# =====================\n",
        "# Training / Evaluation\n",
        "# =====================\n",
        "\n",
        "def train_epoch(model: nn.Module, graph_data: Data, optimizer: torch.optim.Optimizer, criterion: nn.Module, detailed_timing: bool = False) -> Tuple[float, dict]:\n",
        "    \"\"\"Train one epoch with optional detailed timing\"\"\"\n",
        "    model.train()\n",
        "    timings = {}\n",
        "\n",
        "    try:\n",
        "        t0 = time.perf_counter()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if detailed_timing:\n",
        "            timings['zero_grad'] = time.perf_counter() - t0\n",
        "\n",
        "        # Single full-batch forward/backward on the static graph\n",
        "        t0 = time.perf_counter()\n",
        "        out = model(graph_data.x, graph_data.edge_index)\n",
        "        if detailed_timing:\n",
        "            timings['forward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
        "        if detailed_timing:\n",
        "            timings['loss_computation'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        loss.backward()\n",
        "        if detailed_timing:\n",
        "            timings['backward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        if detailed_timing:\n",
        "            timings['grad_clipping'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        optimizer.step()\n",
        "        if detailed_timing:\n",
        "            timings['optimizer_step'] = time.perf_counter() - t0\n",
        "            timings['total'] = sum(timings.values())\n",
        "\n",
        "        return float(loss.item()), timings\n",
        "    except Exception as e:\n",
        "        print(f\"Train step failure: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, graph_data: Data, mask: torch.Tensor, detailed_timing: bool = False) -> Tuple[float, dict]:\n",
        "    \"\"\"Evaluate with optional detailed timing\"\"\"\n",
        "    timings = {}\n",
        "    model.eval()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    if detailed_timing:\n",
        "        timings['forward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    pred = out[mask].argmax(dim=1)\n",
        "    acc = (pred == graph_data.y[mask]).float().mean().item()\n",
        "    if detailed_timing:\n",
        "        timings['prediction'] = time.perf_counter() - t0\n",
        "        timings['total'] = sum(timings.values())\n",
        "\n",
        "    return acc, timings\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_detailed(model: nn.Module, graph_data: Data, mask: torch.Tensor, label_mapping: dict, phase: str = \"Test\") -> dict:\n",
        "    \"\"\"Detailed per-class evaluation (fixed for missing classes)\"\"\"\n",
        "    model.eval()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    pred = out[mask].argmax(dim=1).cpu().numpy()\n",
        "    true = graph_data.y[mask].cpu().numpy()\n",
        "\n",
        "    # Reverse label mapping\n",
        "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    # Only include classes that actually appear in test set\n",
        "    unique_classes = np.unique(np.concatenate([true, pred]))\n",
        "    target_names = [idx_to_label[i] for i in unique_classes]\n",
        "\n",
        "    # Show which classes are missing\n",
        "    all_classes = set(range(len(label_mapping)))\n",
        "    present_classes = set(unique_classes)\n",
        "    missing_classes = all_classes - present_classes\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{phase} Set - Detailed Performance Report\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    if missing_classes:\n",
        "        print(f\"\\n⚠️  WARNING: {len(missing_classes)} classes not present in {phase.lower()} set:\")\n",
        "        for class_idx in sorted(missing_classes):\n",
        "            print(f\"  • {idx_to_label[class_idx]}\")\n",
        "        print(f\"  (This is normal with small sample sizes and rare classes)\")\n",
        "\n",
        "    # Overall accuracy\n",
        "    overall_acc = (pred == true).mean()\n",
        "    print(f\"\\nOverall Accuracy: {overall_acc:.4f}\")\n",
        "    print(f\"Classes evaluated: {len(unique_classes)}/{len(label_mapping)}\")\n",
        "\n",
        "    # Per-class metrics (only for classes present in test set)\n",
        "    print(\"\\nPer-Class Classification Report:\")\n",
        "    print(classification_report(true, pred, labels=unique_classes, target_names=target_names, zero_division=0, digits=4))\n",
        "\n",
        "    # Confusion matrix (abbreviated)\n",
        "    cm = confusion_matrix(true, pred)\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for i, label in enumerate(target_names):\n",
        "        class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0.0\n",
        "        class_samples = cm[i].sum()\n",
        "        print(f\"  {label:30s}: {class_acc:.4f} ({int(class_samples)} samples)\")\n",
        "\n",
        "    # Macro and weighted F1\n",
        "    f1_macro = f1_score(true, pred, average='macro', zero_division=0)\n",
        "    f1_weighted = f1_score(true, pred, average='weighted', zero_division=0)\n",
        "    print(f\"\\nF1 Score (Macro):    {f1_macro:.4f}\")\n",
        "    print(f\"\\nF1 Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': float(overall_acc),\n",
        "        'f1_macro': float(f1_macro),\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    run_started = time.perf_counter()\n",
        "    run_id = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
        "    metrics = {\n",
        "        'version': 'v4.8.1',\n",
        "        'run_id': run_id,\n",
        "        'env': get_env_info(),\n",
        "        'paths': {\n",
        "            'file_path': FILE_PATH,\n",
        "            'model_save_path': MODEL_SAVE_PATH,\n",
        "            'results_path': RESULTS_PATH,\n",
        "        },\n",
        "        'improvements': [\n",
        "            'v4.8.1: PyNNDescent @ 10% data for apples-to-apples vs v4.5!',\n",
        "            'v4.8.1: Testing if approximate KNN affects training quality',\n",
        "            'v4.8.1: ALL settings identical to v4.5 except KNN method',\n",
        "            'v4.8.1: CLASS WEIGHTED LOSS (same as v4.5)',\n",
        "            'v4.8.1: 10% data sampling (283k samples, same as v4.5)',\n",
        "            'PyNNDescent for 10-15x faster KNN (~5s vs ~70s)',\n",
        "            'PCA dimensionality reduction for faster KNN (77 → 20 dims)',\n",
        "            'k=2 neighbors (same as v4.5)',\n",
        "            'Detailed per-class metrics (fixed for missing classes)',\n",
        "            'UHG constraint verification',\n",
        "            'Comprehensive timing instrumentation',\n",
        "            'GPU detection and memory tracking',\n",
        "        ],\n",
        "        'data': {},\n",
        "        'graph': {},\n",
        "        'model': {},\n",
        "        'train': {\n",
        "            'epochs': [],\n",
        "            'best_val': 0.0,\n",
        "            'best_epoch': None,\n",
        "        },\n",
        "        'errors': None,\n",
        "        'timing': {},\n",
        "        'gpu_memory': {},\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Data loading with detailed timing (10% for v4.5 comparison, WITH class weighting)\n",
        "        node_features, labels, label_mapping, class_weights, data_timings = load_and_preprocess_data(FILE_PATH, sample_frac=0.10)\n",
        "\n",
        "        metrics['data'] = {\n",
        "            'num_nodes': int(node_features.size(0)),\n",
        "            'num_features': int(node_features.size(1)),\n",
        "            'num_classes': int(len(label_mapping)),\n",
        "            'sample_fraction': 0.10,\n",
        "            'comparison_target': 'v4.5',\n",
        "            'class_weighted_loss': True,\n",
        "        }\n",
        "        metrics['timing']['data_load'] = data_timings\n",
        "\n",
        "        # Graph construction with detailed timing, PCA, and PyNNDescent\n",
        "        graph_data, graph_timings = create_graph_data(node_features, labels, k=2, use_pca=True, pca_components=20)\n",
        "\n",
        "        metrics['timing']['graph_build'] = graph_timings\n",
        "        metrics['graph'] = {\n",
        "            'num_nodes': int(graph_data.x.size(0)),\n",
        "            'num_edges': int(graph_data.edge_index.size(1)),\n",
        "            'k_neighbors': 2,\n",
        "            'pca_enabled': True,\n",
        "            'pca_components': 20,\n",
        "            'knn_method': 'pynndescent',\n",
        "            'knn_approximate': True,\n",
        "            'train_nodes': int(graph_data.train_mask.sum().item()),\n",
        "            'val_nodes': int(graph_data.val_mask.sum().item()),\n",
        "            'test_nodes': int(graph_data.test_mask.sum().item()),\n",
        "        }\n",
        "\n",
        "        in_channels = graph_data.x.size(1)\n",
        "        hidden_channels = 64\n",
        "        out_channels = len(label_mapping)\n",
        "        num_layers = 2\n",
        "\n",
        "        model = UHGGraphSAGE(in_channels, hidden_channels, out_channels, num_layers).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "        # CLASS-WEIGHTED CrossEntropyLoss for minority class detection\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "        print(f\"\\n✅ Using CLASS-WEIGHTED CrossEntropyLoss (same as v4.5)\")\n",
        "        print(f\"   • Inverse frequency weighting for minority class detection\")\n",
        "        print(f\"   • 10% data (283k samples) - IDENTICAL to v4.5 for comparison\")\n",
        "        print(f\"   • Goal: Test if PyNNDescent (approximate) affects accuracy\")\n",
        "\n",
        "        metrics['model'] = {\n",
        "            'in_channels': in_channels,\n",
        "            'hidden_channels': hidden_channels,\n",
        "            'out_channels': out_channels,\n",
        "            'num_layers': num_layers,\n",
        "            'class_weighted_loss': True,\n",
        "        }\n",
        "\n",
        "        # Track GPU memory before training\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.empty_cache()\n",
        "            mem_allocated_before = torch.cuda.memory_allocated() / 1024**3\n",
        "            mem_reserved_before = torch.cuda.memory_reserved() / 1024**3\n",
        "            print(f\"\\n💾 GPU Memory (before training):\")\n",
        "            print(f\"   • Allocated: {mem_allocated_before:.2f} GB\")\n",
        "            print(f\"   • Reserved:  {mem_reserved_before:.2f} GB\")\n",
        "\n",
        "        print(\"\\nStarting training...\")\n",
        "\n",
        "        best_val_acc = 0.0\n",
        "        best_epoch = 0\n",
        "        patience = 10\n",
        "        epochs_without_improvement = 0\n",
        "        max_epochs = 200\n",
        "\n",
        "        epoch_times = []\n",
        "        train_losses = []\n",
        "        val_accs = []\n",
        "        test_accs = []\n",
        "\n",
        "        for epoch in range(1, max_epochs + 1):\n",
        "            t_epoch_start = time.perf_counter()\n",
        "\n",
        "            # Detailed timing for epochs 1, 2, 50, 100\n",
        "            detailed = (epoch in [1, 2, 50, 100])\n",
        "\n",
        "            loss, train_timings = train_epoch(model, graph_data, optimizer, criterion, detailed_timing=detailed)\n",
        "            val_acc, val_timings = evaluate(model, graph_data, graph_data.val_mask, detailed_timing=detailed)\n",
        "            test_acc, test_timings = evaluate(model, graph_data, graph_data.test_mask, detailed_timing=detailed)\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            epoch_time = time.perf_counter() - t_epoch_start\n",
        "            epoch_times.append(epoch_time)\n",
        "            train_losses.append(loss)\n",
        "            val_accs.append(val_acc)\n",
        "            test_accs.append(test_acc)\n",
        "\n",
        "            improved = val_acc > best_val_acc\n",
        "            if improved:\n",
        "                best_val_acc = val_acc\n",
        "                best_epoch = epoch\n",
        "                epochs_without_improvement = 0\n",
        "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "                improved_str = \"(saved)\"\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                improved_str = \"\"\n",
        "\n",
        "            # Print detailed timing for specific epochs\n",
        "            if detailed:\n",
        "                print(f\"\\n⏱️  Epoch {epoch} Detailed Timing:\")\n",
        "                print(f\"    Train: Forward={train_timings.get('forward_pass', 0):.3f}s, \"\n",
        "                      f\"Backward={train_timings.get('backward_pass', 0):.3f}s, \"\n",
        "                      f\"Optimizer={train_timings.get('optimizer_step', 0):.3f}s\")\n",
        "                print(f\"    Val:   Forward={val_timings.get('forward_pass', 0):.3f}s\")\n",
        "                print(f\"    Test:  Forward={test_timings.get('forward_pass', 0):.3f}s\")\n",
        "\n",
        "            # Print every epoch if improved, or every 10 epochs\n",
        "            if improved or epoch % 10 == 0 or epoch == max_epochs:\n",
        "                print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | Val {val_acc:.4f} | \"\n",
        "                      f\"Test {test_acc:.4f} | LR {current_lr:.5f} | {epoch_time:.2f}s | {improved_str}\")\n",
        "\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\n⏱️  Average epoch time: {np.mean(epoch_times):.2f}s\")\n",
        "\n",
        "        # Track GPU memory after training\n",
        "        if torch.cuda.is_available():\n",
        "            mem_allocated_peak = torch.cuda.max_memory_allocated() / 1024**3\n",
        "            mem_allocated_final = torch.cuda.memory_allocated() / 1024**3\n",
        "            mem_reserved_final = torch.cuda.memory_reserved() / 1024**3\n",
        "\n",
        "            print(f\"\\n💾 GPU Memory Usage Summary:\")\n",
        "            print(f\"   • Peak Allocated: {mem_allocated_peak:.2f} GB\")\n",
        "            print(f\"   • Final Allocated: {mem_allocated_final:.2f} GB\")\n",
        "            print(f\"   • Final Reserved: {mem_reserved_final:.2f} GB\")\n",
        "\n",
        "            metrics['gpu_memory'] = {\n",
        "                'peak_allocated_gb': float(mem_allocated_peak),\n",
        "                'final_allocated_gb': float(mem_allocated_final),\n",
        "                'final_reserved_gb': float(mem_reserved_final),\n",
        "            }\n",
        "\n",
        "        metrics['train']['epochs'] = list(range(1, len(train_losses) + 1))\n",
        "        metrics['train']['losses'] = [float(x) for x in train_losses]\n",
        "        metrics['train']['val_accs'] = [float(x) for x in val_accs]\n",
        "        metrics['train']['test_accs'] = [float(x) for x in test_accs]\n",
        "        metrics['train']['best_val'] = float(best_val_acc)\n",
        "        metrics['train']['best_epoch'] = int(best_epoch)\n",
        "        metrics['train']['total_epochs'] = len(train_losses)\n",
        "        metrics['train']['avg_epoch_time'] = float(np.mean(epoch_times))\n",
        "\n",
        "        # Load best model and evaluate\n",
        "        print(\"\\nLoading best model for final evaluation...\")\n",
        "        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "        # Verify UHG constraints after training\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"POST-TRAINING UHG CONSTRAINT VERIFICATION\")\n",
        "        print(\"=\"*80)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Check constraints after first layer\n",
        "            h = graph_data.x\n",
        "            h = model.layers[0](h, graph_data.edge_index)\n",
        "            verify_uhg_constraints(h, name=\"after layer 1\")\n",
        "\n",
        "        # Final test evaluation with detailed metrics\n",
        "        test_results = evaluate_detailed(model, graph_data, graph_data.test_mask, label_mapping, phase=\"Test\")\n",
        "        metrics['test'] = test_results\n",
        "\n",
        "        print(f\"\\nFinal Test Accuracy: {test_results['accuracy']:.4f}\")\n",
        "\n",
        "        # Comprehensive timing breakdown\n",
        "        run_ended = time.perf_counter()\n",
        "        total_runtime = run_ended - run_started\n",
        "\n",
        "        data_time = data_timings['total']\n",
        "        graph_time = graph_timings['total']\n",
        "        train_time = sum(epoch_times)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"⏱️  COMPREHENSIVE TIMING BREAKDOWN\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\n📊 DATA LOADING ({data_time:.2f}s total):\")\n",
        "        for key, val in data_timings.items():\n",
        "            if key != 'total':\n",
        "                pct = (val / total_runtime) * 100\n",
        "                print(f\"  • {key.replace('_', ' ').title():20s} {val:7.2f}s ({pct:5.1f}%)\")\n",
        "\n",
        "        print(f\"\\n🕸️  GRAPH CONSTRUCTION ({graph_time:.2f}s total):\")\n",
        "        for key, val in graph_timings.items():\n",
        "            if key != 'total':\n",
        "                pct = (val / total_runtime) * 100\n",
        "                if key == 'pca':\n",
        "                    print(f\"  • PCA (77→20 dims): {val:7.2f}s ({pct:5.1f}%)\")\n",
        "                elif key == 'knn_computation':\n",
        "                    print(f\"  • KNN Computation: {val:7.2f}s ({pct:5.1f}%) 🚀 PyNNDescent\")\n",
        "                else:\n",
        "                    print(f\"  • {key.replace('_', ' ').title():20s} {val:7.2f}s ({pct:5.1f}%)\")\n",
        "\n",
        "        print(f\"\\n🎓 TRAINING ({train_time:.2f}s total, {(train_time/total_runtime)*100:.1f}% of runtime):\")\n",
        "        print(f\"  • Avg Epoch Time:      {np.mean(epoch_times):.2f}s\")\n",
        "        print(f\"  • Total Epochs:      {len(epoch_times)}\")\n",
        "\n",
        "        print(f\"\\n📈 HIGH-LEVEL SUMMARY:\")\n",
        "        print(f\"  • Data Loading:       {(data_time/total_runtime)*100:5.1f}% of total runtime\")\n",
        "        print(f\"  • Graph Building:     {(graph_time/total_runtime)*100:5.1f}% of total runtime\")\n",
        "        print(f\"  • Training:           {(train_time/total_runtime)*100:5.1f}% of total runtime\")\n",
        "        print(f\"  • Total Runtime:     {total_runtime:7.2f}s ({total_runtime/60:.1f} min)\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"  • Peak GPU Memory:   {mem_allocated_peak:.2f} GB\")\n",
        "\n",
        "        print(f\"\\n🚀 PYNNDESCENT PERFORMANCE:\")\n",
        "        print(f\"  • KNN Time (PyNNDescent): {graph_timings['knn_computation']:.0f}s\")\n",
        "        print(f\"  • Est. sklearn Time (v4.5): ~70s\")\n",
        "        print(f\"  • Speedup:                ~{70/graph_timings['knn_computation']:.1f}x FASTER! 🚀\")\n",
        "\n",
        "        print(f\"\\n🔬 COMPARISON vs v4.5:\")\n",
        "        print(f\"  • v4.5 @ 10%: sklearn KNN (~70s), 95.51% accuracy\")\n",
        "        print(f\"  • v4.8.1 @ 10%: PyNN KNN ({graph_timings['knn_computation']:.0f}s), {test_results['accuracy']*100:.2f}% accuracy\")\n",
        "        print(f\"  • Speedup: {70/graph_timings['knn_computation']:.1f}x faster KNN\")\n",
        "        print(f\"  • Accuracy delta: {(test_results['accuracy']-0.9551)*100:+.2f}%\")\n",
        "        if abs(test_results['accuracy'] - 0.9551) < 0.005:\n",
        "            print(f\"  ✅ PyNNDescent has NO IMPACT on accuracy (±0.5%)!\")\n",
        "        elif test_results['accuracy'] > 0.9551:\n",
        "            print(f\"  ✅ PyNNDescent is BETTER than sklearn!\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ PyNNDescent trades {(0.9551-test_results['accuracy'])*100:.2f}% accuracy for {70/graph_timings['knn_computation']:.1f}x speed\")\n",
        "\n",
        "        # Bottleneck analysis\n",
        "        all_timings = []\n",
        "        for category, timing_dict in [('Data', data_timings), ('Graph', graph_timings)]:\n",
        "            for key, val in timing_dict.items():\n",
        "                if key != 'total' and val > 0.5:  # Only show > 0.5s\n",
        "                    all_timings.append((f\"{category}: {key}\", val))\n",
        "\n",
        "        all_timings.sort(key=lambda x: x[1], reverse=True)\n",
        "        print(f\"\\n🔍 BOTTLENECK ANALYSIS:\")\n",
        "        for i, (name, time_val) in enumerate(all_timings[:3], 1):\n",
        "            pct = (time_val / total_runtime) * 100\n",
        "            print(f\"  {i}. {name:40s} {time_val:7.2f}s ({pct:5.1f}%)\")\n",
        "\n",
        "        metrics['timing']['total_runtime'] = float(total_runtime)\n",
        "        metrics['timing']['data_time'] = float(data_time)\n",
        "        metrics['timing']['graph_time'] = float(graph_time)\n",
        "        metrics['timing']['train_time'] = float(train_time)\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_file = os.path.join(RESULTS_PATH, f'metrics_v4.8.1_{run_id}.json')\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        print(f\"Saved metrics to: {metrics_file}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"UHG IDS Model v4.8.1 - Training Complete\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Results saved to: {metrics_file}\")\n",
        "        print(f\"\\n🔬 APPLES-TO-APPLES COMPARISON SUCCESS!\")\n",
        "        print(f\"   • Total samples: {metrics['data']['num_nodes']:,}\")\n",
        "        print(f\"   • KNN time: {graph_timings['knn_computation']:.0f}s (vs ~70s sklearn)\")\n",
        "        print(f\"   • Total time: {total_runtime:.0f}s ({total_runtime/60:.1f} min)\")\n",
        "        print(f\"   • PyNNDescent: {70/graph_timings['knn_computation']:.1f}x FASTER than sklearn! 🚀\")\n",
        "        print(f\"   • Accuracy: {test_results['accuracy']*100:.2f}% (v4.5: 95.51%)\")\n",
        "        print(f\"\\n🎯 Now compare this with v4.5 Results 3 to see PyNN's true impact!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Training failed with error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        metrics['errors'] = str(e)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwq9OdADeP-A",
        "outputId": "2e607ea8-988e-4a2f-ce10-c8f25e2b174e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "================================================================================\n",
            "🖥️  HARDWARE CONFIGURATION\n",
            "================================================================================\n",
            "✅ GPU Detected:\n",
            "   • Model: NVIDIA L4\n",
            "   • Memory: 22.2 GB\n",
            "   • CUDA Version: 12.6\n",
            "   • Compute Capability: 8.9\n",
            "   • Device: cuda:0\n",
            "\n",
            "🔬 Configuration (v4.8.1 - Apples-to-Apples vs v4.5):\n",
            "   • KNN Method: PyNNDescent (Approximate) ⭐ TESTING!\n",
            "   • Loss: CLASS-WEIGHTED CrossEntropyLoss ⚖️ (same as v4.5)\n",
            "   • Data: 10% sampling (283k samples) - SAME AS v4.5!\n",
            "   • k=2, PCA, all settings IDENTICAL to v4.5 except KNN\n",
            "   • Expected KNN time: ~4-5s (vs ~70s sklearn in v4.5!)\n",
            "   • Expected total time: ~60-75s (1-1.5 min)\n",
            "   • Goal: Test if PyNN's 2-5% KNN error affects accuracy\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Loading data from: /content/drive/MyDrive/CIC_data.csv\n",
            "  ⏱️  CSV read: 35.89s\n",
            "\n",
            "Unique labels in the dataset: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
            " 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
            " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
            " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n",
            "\n",
            "Label distribution in the dataset:\n",
            "Label\n",
            "BENIGN                        2273097\n",
            "DoS Hulk                       231073\n",
            "PortScan                       158930\n",
            "DDoS                           128027\n",
            "DoS GoldenEye                   10293\n",
            "FTP-Patator                      7938\n",
            "SSH-Patator                      5897\n",
            "DoS slowloris                    5796\n",
            "DoS Slowhttptest                 5499\n",
            "Bot                              1966\n",
            "Web Attack � Brute Force         1507\n",
            "Web Attack � XSS                  652\n",
            "Infiltration                       36\n",
            "Web Attack � Sql Injection         21\n",
            "Heartbleed                         11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Applying random sampling (frac=0.1)...\n",
            "  ⏱️  Sampling: 0.35s\n",
            "\n",
            "Sampled label distribution:\n",
            "Label\n",
            "BENIGN                        227293\n",
            "DoS Hulk                       23097\n",
            "PortScan                       15910\n",
            "DDoS                           12851\n",
            "DoS GoldenEye                   1024\n",
            "FTP-Patator                      825\n",
            "DoS slowloris                    580\n",
            "SSH-Patator                      571\n",
            "DoS Slowhttptest                 525\n",
            "Bot                              194\n",
            "Web Attack � Brute Force         141\n",
            "Web Attack � XSS                  56\n",
            "Infiltration                       3\n",
            "Heartbleed                         2\n",
            "Web Attack � Sql Injection         2\n",
            "Name: count, dtype: int64\n",
            "  ⏱️  Convert to numeric: 0.42s\n",
            "  ⏱️  Fill NaN/inf: 0.84s\n",
            "  ⏱️  Scaling: 0.37s\n",
            "  ⏱️  Convert to tensors: 0.02s\n",
            "\n",
            "Preprocessing complete.\n",
            "Feature shape: torch.Size([283074, 77])\n",
            "Number of unique labels: 15\n",
            "\n",
            "Class distribution in processed data:\n",
            "  BENIGN                        :  227293 samples (80.29%)\n",
            "  Bot                           :     194 samples ( 0.07%)\n",
            "  DDoS                          :   12851 samples ( 4.54%)\n",
            "  DoS GoldenEye                 :    1024 samples ( 0.36%)\n",
            "  DoS Hulk                      :   23097 samples ( 8.16%)\n",
            "  DoS Slowhttptest              :     525 samples ( 0.19%)\n",
            "  DoS slowloris                 :     580 samples ( 0.20%)\n",
            "  FTP-Patator                   :     825 samples ( 0.29%)\n",
            "  Heartbleed                    :       2 samples ( 0.00%)\n",
            "  Infiltration                  :       3 samples ( 0.00%)\n",
            "  PortScan                      :   15910 samples ( 5.62%)\n",
            "  SSH-Patator                   :     571 samples ( 0.20%)\n",
            "  Web Attack � Brute Force      :     141 samples ( 0.05%)\n",
            "  Web Attack � Sql Injection    :       2 samples ( 0.00%)\n",
            "  Web Attack � XSS              :      56 samples ( 0.02%)\n",
            "\n",
            "⚖️  Class Weights (inverse frequency):\n",
            "  BENIGN                        :  0.0830\n",
            "  Bot                           : 97.2763\n",
            "  DDoS                          :  1.4685\n",
            "  DoS GoldenEye                 : 18.4293\n",
            "  DoS Hulk                      :  0.8171\n",
            "  DoS Slowhttptest              : 35.9459\n",
            "  DoS slowloris                 : 32.5372\n",
            "  FTP-Patator                   : 22.8747\n",
            "  Heartbleed                    : 9435.8000\n",
            "  Infiltration                  : 6290.5333\n",
            "  PortScan                      :  1.1861\n",
            "  SSH-Patator                   : 33.0501\n",
            "  Web Attack � Brute Force      : 133.8411\n",
            "  Web Attack � Sql Injection    : 9435.8000\n",
            "  Web Attack � XSS              : 336.9929\n",
            "\n",
            "⏱️  Total data loading time: 38.32s\n",
            "\n",
            "Creating graph structure...\n",
            "\n",
            "Applying PCA for faster KNN...\n",
            "  • Original features: 77\n",
            "  • Reduced features: 20\n",
            "  • Explained variance: 0.9119 (91.19%)\n",
            "  ⏱️  PCA: 0.14s\n",
            "\n",
            "🚀 Computing KNN graph with PyNNDescent (k=2)...\n",
            "  • Input shape: (283074, 20)\n",
            "  • Number of samples: 283,074\n",
            "  • Number of features: 20\n",
            "  • Using PyNNDescent (approximate)\n",
            "  • Algorithm: Nearest Neighbor Descent\n",
            "  • Expected: 10-20x faster than sklearn @ 10%!\n",
            "  • Accuracy: 95-98% of exact KNN\n",
            "  • Building NNDescent index...\n",
            "  ✅ PyNNDescent KNN computation: 22.23s\n",
            "  💡 Speedup vs sklearn (est ~70s @ v4.5): ~3.1x!\n",
            "Edge index shape: torch.Size([2, 566148])\n",
            "  ⏱️  UHG projection: 0.19s\n",
            "Feature shape with homogeneous coordinate: torch.Size([283074, 78])\n",
            "UHG Constraint Check (initial features):\n",
            "  Max violation: 0.062500\n",
            "  Mean violation: 0.000003\n",
            "  ⚠️ WARNING: Constraints violated!\n",
            "\n",
            "Train size: 198151, Val size: 42461, Test size: 42462\n",
            "\n",
            "⏱️  Total graph construction time: 22.78s\n",
            "\n",
            "✅ Using CLASS-WEIGHTED CrossEntropyLoss (same as v4.5)\n",
            "   • Inverse frequency weighting for minority class detection\n",
            "   • 10% data (283k samples) - IDENTICAL to v4.5 for comparison\n",
            "   • Goal: Test if PyNNDescent (approximate) affects accuracy\n",
            "\n",
            "💾 GPU Memory (before training):\n",
            "   • Allocated: 0.09 GB\n",
            "   • Reserved:  0.11 GB\n",
            "\n",
            "Starting training...\n",
            "\n",
            "⏱️  Epoch 1 Detailed Timing:\n",
            "    Train: Forward=0.322s, Backward=0.426s, Optimizer=0.170s\n",
            "    Val:   Forward=0.003s\n",
            "    Test:  Forward=0.003s\n",
            "Epoch 001 | Loss 4.0368 | Val 0.0706 | Test 0.0705 | LR 0.01000 | 1.27s | (saved)\n",
            "\n",
            "⏱️  Epoch 2 Detailed Timing:\n",
            "    Train: Forward=0.003s, Backward=0.004s, Optimizer=0.001s\n",
            "    Val:   Forward=0.003s\n",
            "    Test:  Forward=0.003s\n",
            "Epoch 002 | Loss 2.6062 | Val 0.1385 | Test 0.1375 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 003 | Loss 1.9393 | Val 0.2337 | Test 0.2330 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 004 | Loss 1.5680 | Val 0.3594 | Test 0.3581 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 005 | Loss 1.3250 | Val 0.4530 | Test 0.4509 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 006 | Loss 1.1898 | Val 0.4864 | Test 0.4849 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 007 | Loss 1.0331 | Val 0.5167 | Test 0.5163 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 008 | Loss 0.9141 | Val 0.5492 | Test 0.5480 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 009 | Loss 0.8672 | Val 0.5674 | Test 0.5662 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 010 | Loss 0.7867 | Val 0.5856 | Test 0.5830 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 011 | Loss 0.7323 | Val 0.6055 | Test 0.6039 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 012 | Loss 0.6794 | Val 0.6292 | Test 0.6273 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 013 | Loss 0.6433 | Val 0.6417 | Test 0.6400 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 014 | Loss 0.6254 | Val 0.6510 | Test 0.6485 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 015 | Loss 0.5970 | Val 0.6548 | Test 0.6527 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 017 | Loss 0.5469 | Val 0.6636 | Test 0.6619 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 018 | Loss 0.5406 | Val 0.6770 | Test 0.6745 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 019 | Loss 0.5088 | Val 0.6945 | Test 0.6936 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 020 | Loss 0.4767 | Val 0.7168 | Test 0.7147 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 021 | Loss 0.4778 | Val 0.7350 | Test 0.7331 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 022 | Loss 0.4478 | Val 0.7518 | Test 0.7502 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 023 | Loss 0.4548 | Val 0.7608 | Test 0.7591 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 024 | Loss 0.4321 | Val 0.7647 | Test 0.7630 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 025 | Loss 0.4120 | Val 0.7659 | Test 0.7650 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 026 | Loss 0.4097 | Val 0.7673 | Test 0.7669 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 027 | Loss 0.4057 | Val 0.7689 | Test 0.7684 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 028 | Loss 0.3798 | Val 0.7727 | Test 0.7737 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 029 | Loss 0.3782 | Val 0.7797 | Test 0.7806 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 030 | Loss 0.3826 | Val 0.7853 | Test 0.7869 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 031 | Loss 0.3525 | Val 0.7914 | Test 0.7928 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 032 | Loss 0.3697 | Val 0.7963 | Test 0.7972 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 033 | Loss 0.3536 | Val 0.7999 | Test 0.7996 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 034 | Loss 0.3463 | Val 0.8017 | Test 0.8014 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 038 | Loss 0.3207 | Val 0.8060 | Test 0.8062 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 039 | Loss 0.3245 | Val 0.8126 | Test 0.8133 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 040 | Loss 0.3108 | Val 0.8180 | Test 0.8186 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 041 | Loss 0.3037 | Val 0.8242 | Test 0.8241 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 042 | Loss 0.3212 | Val 0.8247 | Test 0.8254 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 046 | Loss 0.2898 | Val 0.8249 | Test 0.8246 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 047 | Loss 0.2886 | Val 0.8265 | Test 0.8264 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 048 | Loss 0.2894 | Val 0.8284 | Test 0.8283 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 049 | Loss 0.2827 | Val 0.8308 | Test 0.8306 | LR 0.01000 | 0.20s | (saved)\n",
            "\n",
            "⏱️  Epoch 50 Detailed Timing:\n",
            "    Train: Forward=0.003s, Backward=0.003s, Optimizer=0.001s\n",
            "    Val:   Forward=0.002s\n",
            "    Test:  Forward=0.002s\n",
            "Epoch 050 | Loss 0.2823 | Val 0.8340 | Test 0.8338 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 051 | Loss 0.2789 | Val 0.8352 | Test 0.8351 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 053 | Loss 0.2729 | Val 0.8366 | Test 0.8361 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 054 | Loss 0.2658 | Val 0.8382 | Test 0.8378 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 055 | Loss 0.2787 | Val 0.8396 | Test 0.8387 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 056 | Loss 0.2695 | Val 0.8412 | Test 0.8401 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 057 | Loss 0.2699 | Val 0.8421 | Test 0.8403 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 058 | Loss 0.2643 | Val 0.8427 | Test 0.8404 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 060 | Loss 0.2636 | Val 0.8413 | Test 0.8395 | LR 0.01000 | 0.20s | \n",
            "Epoch 064 | Loss 0.2604 | Val 0.8453 | Test 0.8442 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 065 | Loss 0.2621 | Val 0.8469 | Test 0.8454 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 066 | Loss 0.2607 | Val 0.8492 | Test 0.8474 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 070 | Loss 0.2478 | Val 0.8506 | Test 0.8486 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 071 | Loss 0.2498 | Val 0.8524 | Test 0.8503 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 072 | Loss 0.2403 | Val 0.8540 | Test 0.8517 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 073 | Loss 0.2596 | Val 0.8564 | Test 0.8540 | LR 0.01000 | 0.19s | (saved)\n",
            "Epoch 074 | Loss 0.2549 | Val 0.8577 | Test 0.8554 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 075 | Loss 0.2402 | Val 0.8588 | Test 0.8565 | LR 0.01000 | 0.20s | (saved)\n",
            "Epoch 080 | Loss 0.2511 | Val 0.8562 | Test 0.8546 | LR 0.01000 | 0.19s | \n",
            "Epoch 085 | Loss 0.2270 | Val 0.8592 | Test 0.8578 | LR 0.00500 | 0.20s | (saved)\n",
            "Epoch 086 | Loss 0.2274 | Val 0.8597 | Test 0.8582 | LR 0.00500 | 0.20s | (saved)\n",
            "Epoch 087 | Loss 0.2262 | Val 0.8598 | Test 0.8582 | LR 0.00500 | 0.20s | (saved)\n",
            "Epoch 090 | Loss 0.2319 | Val 0.8607 | Test 0.8591 | LR 0.00500 | 0.20s | (saved)\n",
            "Epoch 091 | Loss 0.2260 | Val 0.8614 | Test 0.8595 | LR 0.00500 | 0.20s | (saved)\n",
            "Epoch 092 | Loss 0.2252 | Val 0.8617 | Test 0.8600 | LR 0.00500 | 0.20s | (saved)\n",
            "\n",
            "⏱️  Epoch 100 Detailed Timing:\n",
            "    Train: Forward=0.002s, Backward=0.004s, Optimizer=0.001s\n",
            "    Val:   Forward=0.002s\n",
            "    Test:  Forward=0.002s\n",
            "Epoch 100 | Loss 0.2254 | Val 0.8586 | Test 0.8577 | LR 0.00250 | 0.20s | \n",
            "Early stopping.\n",
            "\n",
            "⏱️  Average epoch time: 0.21s\n",
            "\n",
            "💾 GPU Memory Usage Summary:\n",
            "   • Peak Allocated: 1.56 GB\n",
            "   • Final Allocated: 0.11 GB\n",
            "   • Final Reserved: 1.99 GB\n",
            "\n",
            "Loading best model for final evaluation...\n",
            "\n",
            "================================================================================\n",
            "POST-TRAINING UHG CONSTRAINT VERIFICATION\n",
            "================================================================================\n",
            "UHG Constraint Check (after layer 1):\n",
            "  Max violation: 3.000000\n",
            "  Mean violation: 0.000042\n",
            "  ⚠️ WARNING: Constraints violated!\n",
            "\n",
            "================================================================================\n",
            "Test Set - Detailed Performance Report\n",
            "================================================================================\n",
            "\n",
            "Overall Accuracy: 0.8600\n",
            "Classes evaluated: 15/15\n",
            "\n",
            "Per-Class Classification Report:\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN     0.9986    0.8309    0.9071     34104\n",
            "                       Bot     0.0195    1.0000    0.0382        33\n",
            "                      DDoS     0.6827    0.9989    0.8111      1900\n",
            "             DoS GoldenEye     0.2559    0.9939    0.4070       164\n",
            "                  DoS Hulk     0.8446    0.9666    0.9015      3503\n",
            "          DoS Slowhttptest     0.5789    0.9778    0.7273        90\n",
            "             DoS slowloris     0.2521    0.9888    0.4018        89\n",
            "               FTP-Patator     0.2470    0.9841    0.3949       126\n",
            "                Heartbleed     0.0000    0.0000    0.0000         1\n",
            "              Infiltration     0.0000    0.0000    0.0000         1\n",
            "                  PortScan     0.8142    0.9889    0.8931      2344\n",
            "               SSH-Patator     0.1268    0.8625    0.2212        80\n",
            "  Web Attack � Brute Force     0.0759    0.3000    0.1212        20\n",
            "Web Attack � Sql Injection     0.0000    0.0000    0.0000         0\n",
            "          Web Attack � XSS     0.0240    1.0000    0.0468         7\n",
            "\n",
            "                  accuracy                         0.8600     42462\n",
            "                 macro avg     0.3280    0.7262    0.3914     42462\n",
            "              weighted avg     0.9510    0.8600    0.8941     42462\n",
            "\n",
            "\n",
            "Per-Class Accuracy:\n",
            "  BENIGN                        : 0.8309 (34104 samples)\n",
            "  Bot                           : 1.0000 (33 samples)\n",
            "  DDoS                          : 0.9989 (1900 samples)\n",
            "  DoS GoldenEye                 : 0.9939 (164 samples)\n",
            "  DoS Hulk                      : 0.9666 (3503 samples)\n",
            "  DoS Slowhttptest              : 0.9778 (90 samples)\n",
            "  DoS slowloris                 : 0.9888 (89 samples)\n",
            "  FTP-Patator                   : 0.9841 (126 samples)\n",
            "  Heartbleed                    : 0.0000 (1 samples)\n",
            "  Infiltration                  : 0.0000 (1 samples)\n",
            "  PortScan                      : 0.9889 (2344 samples)\n",
            "  SSH-Patator                   : 0.8625 (80 samples)\n",
            "  Web Attack � Brute Force      : 0.3000 (20 samples)\n",
            "  Web Attack � Sql Injection    : 0.0000 (0 samples)\n",
            "  Web Attack � XSS              : 1.0000 (7 samples)\n",
            "\n",
            "F1 Score (Macro):    0.3914\n",
            "\n",
            "F1 Score (Weighted): 0.8941\n",
            "\n",
            "Final Test Accuracy: 0.8600\n",
            "\n",
            "================================================================================\n",
            "⏱️  COMPREHENSIVE TIMING BREAKDOWN\n",
            "================================================================================\n",
            "\n",
            "📊 DATA LOADING (38.32s total):\n",
            "  • Csv Read               35.89s ( 42.5%)\n",
            "  • Column Cleanup          0.43s (  0.5%)\n",
            "  • Sampling                0.35s (  0.4%)\n",
            "  • To Numeric              0.42s (  0.5%)\n",
            "  • Fillna                  0.84s (  1.0%)\n",
            "  • Scaling                 0.37s (  0.4%)\n",
            "  • To Tensors              0.02s (  0.0%)\n",
            "\n",
            "🕸️  GRAPH CONSTRUCTION (22.78s total):\n",
            "  • To Numpy                0.00s (  0.0%)\n",
            "  • PCA (77→20 dims):    0.14s (  0.2%)\n",
            "  • KNN Computation:   22.23s ( 26.3%) 🚀 PyNNDescent\n",
            "  • Edge Index Creation     0.00s (  0.0%)\n",
            "  • Add Homogeneous         0.07s (  0.1%)\n",
            "  • Projective Normalize    0.19s (  0.2%)\n",
            "  • Constraint Verification    0.10s (  0.1%)\n",
            "  • Split Creation          0.04s (  0.1%)\n",
            "\n",
            "🎓 TRAINING (20.99s total, 24.9% of runtime):\n",
            "  • Avg Epoch Time:      0.21s\n",
            "  • Total Epochs:      102\n",
            "\n",
            "📈 HIGH-LEVEL SUMMARY:\n",
            "  • Data Loading:        45.4% of total runtime\n",
            "  • Graph Building:      27.0% of total runtime\n",
            "  • Training:            24.9% of total runtime\n",
            "  • Total Runtime:       84.38s (1.4 min)\n",
            "  • Peak GPU Memory:   1.56 GB\n",
            "\n",
            "🚀 PYNNDESCENT PERFORMANCE:\n",
            "  • KNN Time (PyNNDescent): 22s\n",
            "  • Est. sklearn Time (v4.5): ~70s\n",
            "  • Speedup:                ~3.1x FASTER! 🚀\n",
            "\n",
            "🔬 COMPARISON vs v4.5:\n",
            "  • v4.5 @ 10%: sklearn KNN (~70s), 95.51% accuracy\n",
            "  • v4.8.1 @ 10%: PyNN KNN (22s), 86.00% accuracy\n",
            "  • Speedup: 3.1x faster KNN\n",
            "  • Accuracy delta: -9.51%\n",
            "  ⚠️ PyNNDescent trades 9.51% accuracy for 3.1x speed\n",
            "\n",
            "🔍 BOTTLENECK ANALYSIS:\n",
            "  1. Data: csv_read                             35.89s ( 42.5%)\n",
            "  2. Graph: knn_computation                     22.23s ( 26.3%)\n",
            "  3. Data: fillna                                0.84s (  1.0%)\n",
            "Saved metrics to: /content/drive/MyDrive/uhg_ids_results/metrics_v4.8.1_20251004T124644.json\n",
            "\n",
            "================================================================================\n",
            "UHG IDS Model v4.8.1 - Training Complete\n",
            "================================================================================\n",
            "Results saved to: /content/drive/MyDrive/uhg_ids_results/metrics_v4.8.1_20251004T124644.json\n",
            "\n",
            "🔬 APPLES-TO-APPLES COMPARISON SUCCESS!\n",
            "   • Total samples: 283,074\n",
            "   • KNN time: 22s (vs ~70s sklearn)\n",
            "   • Total time: 84s (1.4 min)\n",
            "   • PyNNDescent: 3.1x FASTER than sklearn! 🚀\n",
            "   • Accuracy: 86.00% (v4.5: 95.51%)\n",
            "\n",
            "🎯 Now compare this with v4.5 Results 3 to see PyNN's true impact!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VibOCFcGeTX7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}